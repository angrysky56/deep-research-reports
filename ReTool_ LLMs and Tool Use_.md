# **ReTool: Enhancing Large Language Model Reasoning Through Reinforcement Learning and Strategic Tool Use**

The landscape of large language models (LLMs) has witnessed remarkable advancements in textual reasoning. However, these models often encounter limitations when confronted with tasks demanding structured problem-solving, particularly those involving geometric reasoning, concise computation, or the resolution of complex equations.1 In such scenarios, computational tools like code interpreters demonstrate a distinct advantage. Recognizing this gap, a recent paper titled "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs" introduces a novel framework designed to enhance the reasoning capabilities of LLMs by strategically integrating computational tools, specifically code interpreters.1 This framework, known as ReTool, incorporates two pivotal features: the dynamic interleaving of real-time code execution within natural language reasoning processes and an automated reinforcement learning (RL) paradigm that enables the model to learn when and how to effectively utilize tools based on the outcomes of their use.1 The research demonstrates significant performance improvements on challenging benchmarks, alongside the emergence of sophisticated behaviors like code self-correction, suggesting a promising direction for advancing complex reasoning and the development of hybrid neuro-symbolic systems.1  
While LLMs have shown impressive abilities in understanding and generating human language, their capacity for structured reasoning, especially in domains requiring precise calculations or symbolic manipulation, remains a challenge.1 Tasks such as geometric proofs, intricate numerical computations, and solving complex algebraic equations often demand a level of accuracy and systematic processing that purely text-based reasoning can struggle to provide.1 Furthermore, when dealing with long chains of reasoning, the potential for ambiguity and the accumulation of errors can significantly impact the reliability of the final answer.5 In contrast, computational tools, such as code interpreters, are inherently adept at handling structured problems and delivering accurate results.1 These tools enable exact numeric validation of intermediate steps, thereby reducing ambiguity and the compounding of errors often observed in textual reasoning.5 Moreover, they allow for a more expansive exploration of potential solutions through programmable exploration.5 Previous attempts to integrate tools with LLMs have explored methods that combine textual reasoning with code execution.7 However, these approaches often rely on supervised learning, which can limit the model's ability to generalize beyond the specific patterns present in the training data.7 Additionally, earlier efforts to apply reinforcement learning for tool use in LLMs have not always yielded optimal results.8 This context highlights the novelty of ReTool's approach in leveraging reinforcement learning for the strategic and adaptive use of computational tools.  
ReTool introduces a novel reinforcement learning framework explicitly designed to guide LLMs toward optimal strategies for utilizing external computational tools during the reasoning process.6 The framework fundamentally models the use of tools as an integral part of the decision-making process within the LLM.6 ReTool achieves this through two key features. First, it enables the **dynamic interleaving of code execution** within the natural language reasoning process.1 This means that during its reasoning, the model can generate and execute code snippets using a code interpreter and subsequently incorporate the results back into its ongoing reasoning steps.4 This capability allows the model to perform precise numerical calculations and symbolic manipulations, which are often challenging for purely text-based models.7 Second, ReTool employs an **automated reinforcement learning (RL) paradigm for tool use**.1 Through this paradigm, the model learns when and how to use tools effectively based on the feedback received from the outcome of its actions.1 The RL process involves multi-turn interactions with the code interpreter during policy rollouts, where the model is taught through outcome-based feedback, specifically the correctness of the final answer.1 This approach allows the model to autonomously discover effective patterns for invoking tools without relying on predefined human rules.1  
The dynamic interleaving of code execution in ReTool allows the LLM to seamlessly integrate computational steps into its reasoning process. When the model encounters a part of the problem that requires calculation or symbolic manipulation, it can generate a code snippet, typically enclosed within \<code\> tags.4 This code is then executed by a sandboxed code interpreter, and the output, which can include successful results or error messages, is fed back to the model within \<interpreter\> tags.4 This direct feedback loop enables the model to leverage the precision of code for tasks where textual reasoning might be insufficient.4 Furthermore, this capability allows the model to perform exact numeric validation of intermediate steps, enhancing the accuracy and reliability of its reasoning.5 The automated RL paradigm for tool use centers around the LLM, acting as a policy, interacting with the problem and the code interpreter over multiple turns.1 The model receives a reward based on whether its final answer to the problem is correct.4 This outcome-driven reward system encourages the model to explore various strategies for using the code interpreter.4 By focusing on the final outcome, the model learns to decide the most effective timing and method for invoking the code interpreter during its reasoning process, ultimately optimizing its problem-solving capabilities.5  
The training of ReTool follows a two-stage methodology designed to first equip the LLM with foundational tool-using skills and then refine these skills through reinforcement learning.4 The first stage involves **Cold-Start Supervised Fine-Tuning (SFT)**. This stage begins with the creation of a high-quality cold-start dataset.4 This dataset is constructed by automatically transforming existing mathematical reasoning data to include code-integrated reasoning traces.4 In this transformation, manual calculation steps within the reasoning process are replaced with corresponding code snippets and their execution results obtained from the code interpreter.4 To ensure the quality of this generated data, a two-stage verification protocol is applied.4 The first stage, format verification, checks for readability and consistent syntax, which facilitates the detection of tool invocation triggers during the subsequent RL phase.4 The second stage, answer verification, eliminates samples where the final output from the code-augmented reasoning does not match the correct solution.4 The base LLM is then fine-tuned on this carefully curated dataset.4 The goal of this SFT stage is to provide the model with an initial understanding of when and how to use the code interpreter effectively.4 Notably, the performance of the model after this cold-start training is comparable to that of traditional RL baselines, highlighting the effectiveness of the curated dataset in capturing useful tool usage patterns.7 The second stage of training is **Reinforcement Learning (RL) with Interleaved Code Execution**.4 ReTool utilizes the Proximal Policy Optimization (PPO) algorithm for this RL stage.4 The reward design is based on a simple rule-based accuracy metric: the model receives a reward of \+1 for a correct final answer and \-1 otherwise.4 For reliable verification, the model is required to present its final answer in a specific format, enclosed within \\boxed{}.4 This focus on the outcome encourages the model to autonomously discover effective problem-solving strategies that involve tool use.4 During RL training, the policy LLM interacts with a sandboxed code interpreter to generate reasoning trajectories.4 The model generates natural language reasoning, and when it needs to perform a computation, it generates a code snippet within \<code\> tags.4 This code is then executed, and the output, including both successful results and error messages, is fed back to the model within \<interpreter\> tags.4 The model then continues its reasoning process, potentially generating more code or providing the final answer. This interleaved process allows for dynamic adaptation based on the real-time feedback from the tool.4 The training process leverages the VeRL framework with specific hyperparameters, including the AdamW optimizer and a defined maximum sequence length.4 Techniques such as masking out the interpreter feedback from the loss computation and KV-cache reuse are employed to improve training stability and efficiency.4 Furthermore, an asynchronous code sandbox environment is used to accelerate the RL training by allowing parallel code execution.4  
To validate the effectiveness of the ReTool framework, the authors conducted experiments on the challenging MATH Olympiad benchmark, specifically the AIME 2024 and AIME 2025 datasets.1 The AIME benchmark is known for its difficulty and requires advanced mathematical reasoning skills.1 The results demonstrated a significant improvement in accuracy achieved by ReTool compared to several baseline models, including a text-based reinforcement learning approach.4 Using the Qwen2.5-32B-Instruct model as its backbone, ReTool achieved an accuracy of **67.0%** on AIME 2024 with only 400 training steps.1 This performance substantially outperformed the text-based RL baseline, which only reached **40.0%** accuracy even after 1080 training steps.1 On the AIME 2025 dataset, ReTool (Qwen2.5-32B-Instruct) achieved an accuracy of **49.3%**, again significantly better than the text-based RL baseline's score of **36.7%**.4 Further performance enhancements were observed when ReTool was implemented with a more advanced backbone, the DeepSeek-R1-Distill-Qwen-32B model, reaching **72.5%** accuracy on AIME 2024 and **54.3%** on AIME 2025\.4 The following table summarizes the performance comparison of ReTool against several existing baseline models on the AIME benchmarks 4:

| Model | AIME2024 (pass@1) | AIME2025 (pass@1) |
| :---- | :---- | :---- |
| Qwen2.5-Math-72B-Instruct | 30.0 | \- |
| Qwen2.5-Math-72B-Instruct-TIR | 40.0 | \- |
| Sky-T1 | 43.3 | \- |
| OpenAI o1-preview | 44.6 | 37.9 |
| DeepSeek-R1-Zero-Qwen-32B | 47.0 | \- |
| QWQ-32B-Preview | 50.0 | 33.5 |
| s1-32B | 56.7 | \- |
| **ReTool (Qwen2.5-32B-Instruct)** | **67.0** | **49.3** |
| **ReTool (DeepSeek-R1-Distill-Qwen-32B)** | **72.5** | **54.3** |
| w/o Training (Base Model) | 26.7 | \- |
| w/o CI (Text-based RL) | 40.0 | 36.7 |
| w/o RL (only Cold-start) | 40.9 | 34.5 |

These results clearly demonstrate the superiority of the ReTool framework over the listed baseline models on both AIME datasets. For instance, ReTool (Qwen2.5-32B-Instruct) outperformed the competitive s1-32B by 10.3% on AIME 2024 and achieved an 11.4% gain over OpenAI’s o1-preview on AIME 2025\.4 Ablation studies further highlighted the importance of different components of ReTool. The model without any training performed the worst, while the model trained with text-based RL showed improvement but was significantly less effective than ReTool. Similarly, the model with only cold-start training performed comparably to the text-based RL baseline, underscoring the crucial role of the RL stage in ReTool's success.4  
Beyond the significant performance gains, the ReTool framework exhibits interesting "emergent behaviors," one of the most notable being code self-correction.1 This refers to the model's ability to identify and rectify errors in the code it generates during the reasoning process, without being explicitly trained to do so.1 For example, the paper highlights a scenario where the model initially produced Python code containing an undefined function. Upon receiving feedback from the code interpreter in the form of a NameError traceback, the model demonstrated an understanding of the error. It reflected on the issue, stating that the functions needed to be defined in the same scope, and then proceeded to generate a corrected version of the code that included the necessary function definition.4 This emergent ability to self-correct has several important implications. It enhances the model's problem-solving capabilities by allowing it to overcome initial mistakes in code generation, leading to more robust and accurate solutions.4 It also suggests the development of metacognitive skills within the model, where it can monitor and revise its own reasoning and execution process.4 Furthermore, this autonomous refinement of code can potentially lead to more efficient problem-solving.4 The emergence of this behavior is attributed to the reinforcement learning paradigm employed in ReTool. The outcome-based rewards incentivize the model to generate correct solutions. When initial code fails and does not lead towards a correct answer, the model learns to adjust its strategy, which in this case manifests as self-correction. The feedback from the code interpreter plays a vital role in guiding the model to identify and fix these errors.4 The fact that this self-correction ability arises without explicit training data for such scenarios underscores the power of reinforcement learning in fostering complex and adaptive behaviors in LLMs.4 It suggests that by focusing on the desired outcome (correctly solving the problem), RL can lead to the development of sophisticated internal mechanisms for achieving that outcome.  
The release of the "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs" paper in April 2025 has garnered attention within the AI research community. The project page associated with the paper indicates that the codebase and trained model checkpoints will be open-sourced soon, a move generally welcomed by the research community as it facilitates further exploration and development.9 The paper's availability on arXiv and Hugging Face provides broad access to the research findings.1 Online discussions on platforms like Substack and YouTube highlight the perceived novelty and practical utility of the ReTool method, particularly its two-stage training process involving cold-start supervised fine-tuning followed by reinforcement learning.8 The significant performance gains achieved on the challenging AIME benchmark are consistently noted as a key strength of the approach.7 The emergent behaviors, especially the model's ability to perform code self-correction, are considered particularly noteworthy and indicative of advanced learning capabilities.7 Some discussions also raise potential limitations, such as the framework's reliance on the quality of the initial cold-start dataset and the effectiveness of the underlying code interpreter.13 The question of how well ReTool might generalize to other types of tools and reasoning domains beyond mathematics is also a subject of interest.12 Overall, the initial reception suggests that ReTool is viewed as a significant step forward in the development of more capable and adaptable LLMs, representing a promising direction for integrating neural and symbolic approaches to artificial intelligence.12  
The success of ReTool carries significant implications for the future trajectory of LLMs and the advancement of neuro-symbolic AI systems. By demonstrating an effective method for LLMs to strategically utilize external tools, ReTool offers a promising pathway to overcome the inherent limitations of purely text-based reasoning.1 The ability to leverage computational tools can substantially expand the range of complex tasks that LLMs can tackle with accuracy and efficiency.12 The outcome-driven approach to tool integration, as implemented in ReTool, has the potential to lead to more robust and reliable solutions for intricate reasoning problems.1 Furthermore, the observed reduction in response length after RL training suggests an improvement in reasoning token efficiency, which is crucial for practical applications of LLMs.5 ReTool also represents a significant contribution to the field of hybrid neuro-symbolic AI.1 It effectively combines the pattern recognition strengths of neural networks with the precision and reliability of symbolic tools, such as code interpreters.1 The framework demonstrates how reinforcement learning can be used to learn an effective blend of these two paradigms based on task outcomes.12 This integration has the potential to yield AI systems that are not only powerful in their performance but also more interpretable and reliable in their decision-making processes.15 The principles demonstrated by ReTool in the domain of mathematical reasoning could potentially be extended to various other domains requiring structured reasoning combined with external computation or information retrieval, such as scientific discovery, planning tasks using databases, and even software development.12  
In conclusion, the paper "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs" addresses the critical challenge of enhancing the reasoning capabilities of large language models beyond the constraints of purely textual processing. The proposed ReTool framework introduces a novel approach that leverages the dynamic interleaving of code execution and an automated reinforcement learning paradigm to enable LLMs to strategically utilize computational tools. The experimental results on the challenging AIME benchmark demonstrate significant performance gains and improved training efficiency compared to baseline and state-of-the-art models. The emergence of sophisticated behaviors like code self-correction further underscores the advanced learning capabilities fostered by the ReTool framework. Ultimately, this work highlights the significant potential of outcome-driven tool integration for advancing complex reasoning in LLMs and makes a valuable contribution to the ongoing development of more powerful and versatile neuro-symbolic AI systems.

#### **Works cited**

1. \[2504.11536\] ReTool: Reinforcement Learning for Strategic Tool Use in LLMs \- arXiv, accessed April 25, 2025, [https://arxiv.org/abs/2504.11536](https://arxiv.org/abs/2504.11536)  
2. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs \- Hugging Face, accessed April 25, 2025, [https://huggingface.co/papers/2504.11536](https://huggingface.co/papers/2504.11536)  
3. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs \- ChatPaper, accessed April 25, 2025, [https://chatpaper.com/chatpaper/zh-CN/paper/130255](https://chatpaper.com/chatpaper/zh-CN/paper/130255)  
4. arxiv.org, accessed April 25, 2025, [https://arxiv.org/pdf/2504.11536](https://arxiv.org/pdf/2504.11536)  
5. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs \- arXiv, accessed April 25, 2025, [https://www.arxiv.org/pdf/2504.11536](https://www.arxiv.org/pdf/2504.11536)  
6. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs \- arXiv, accessed April 25, 2025, [https://arxiv.org/html/2504.11536v1](https://arxiv.org/html/2504.11536v1)  
7. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs \- Powerdrill, accessed April 25, 2025, [https://powerdrill.ai/discover/summary-retool-reinforcement-learning-for-strategic-tool-cm9lujdlp7khi07rauf826uin](https://powerdrill.ai/discover/summary-retool-reinforcement-learning-for-strategic-tool-cm9lujdlp7khi07rauf826uin)  
8. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs \- YouTube, accessed April 25, 2025, [https://www.youtube.com/watch?v=WdJbXUOI2Ag](https://www.youtube.com/watch?v=WdJbXUOI2Ag)  
9. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs, accessed April 25, 2025, [https://retool-rl.github.io/](https://retool-rl.github.io/)  
10. Note \- Home | Substack, accessed April 25, 2025, [https://substack.com/@interestingengineering/note/c-109865683?](https://substack.com/@interestingengineering/note/c-109865683)  
11. Interesting Engineering \++ | Substack, accessed April 25, 2025, [https://substack.com/@interestingengineering](https://substack.com/@interestingengineering)  
12. ReTool: RL for Strategic Tool Use in LLMs (Apr 2025\) \- YouTube, accessed April 25, 2025, [https://www.youtube.com/watch?v=O\_35NP5rWig](https://www.youtube.com/watch?v=O_35NP5rWig)  
13. \[QA\] ReTool: Reinforcement Learning for Strategic Tool Use in LLMs \- YouTube, accessed April 25, 2025, [https://www.youtube.com/watch?v=4YC6cAD3ai4](https://www.youtube.com/watch?v=4YC6cAD3ai4)  
14. Daily Papers \- Hugging Face, accessed April 25, 2025, [https://huggingface.co/papers?q=natural%20language%20reasoning%20processes](https://huggingface.co/papers?q=natural+language+reasoning+processes)  
15. Neuro-Symbolic AI: Why Is It The Future of Artificial Intelligence \- Startup Kitchen, accessed April 25, 2025, [https://startupkitchen.community/neuro-symbolic-ai-why-is-it-the-future-of-artificial-intelligence/](https://startupkitchen.community/neuro-symbolic-ai-why-is-it-the-future-of-artificial-intelligence/)  
16. Title: Advanced AI in Banking: Strategic Applications, Emerging Technologies, and Institutional Transformation \- ResearchGate, accessed April 25, 2025, [https://www.researchgate.net/publication/390625969\_Title\_Advanced\_AI\_in\_Banking\_Strategic\_Applications\_Emerging\_Technologies\_and\_Institutional\_Transformation](https://www.researchgate.net/publication/390625969_Title_Advanced_AI_in_Banking_Strategic_Applications_Emerging_Technologies_and_Institutional_Transformation)  
17. This AI Paper from UC Berkeley Shows How Interfacing GPT with Prolog (Reliable Symbolic System) Drastically Improves Its Math Problem-Solving Abilities \- MarkTechPost, accessed April 25, 2025, [https://www.marktechpost.com/2024/07/22/this-ai-paper-from-uc-berkeley-shows-how-interfacing-gpt-with-prolog-reliable-symbolic-system-drastically-improves-its-math-problem-solving-abilities/](https://www.marktechpost.com/2024/07/22/this-ai-paper-from-uc-berkeley-shows-how-interfacing-gpt-with-prolog-reliable-symbolic-system-drastically-improves-its-math-problem-solving-abilities/)