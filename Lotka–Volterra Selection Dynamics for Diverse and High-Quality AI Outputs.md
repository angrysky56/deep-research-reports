
# Lotka–Volterra Selection Dynamics for Diverse and High-Quality AI Outputs

## Abstract

Modern large language models (LLMs) and multi-agent AI systems often suffer from **premature convergence** in their outputs or strategies. Decoding methods that purely maximize log-likelihood (e.g. beam search) tend to produce degenerate, repetitive, or overly generic text ([The Curious Case of Neural Text Degeneration | OpenReview](https://openreview.net/forum?id=rygGQyrFvH#:~:text=language%20model%20%28e,distribution%2C%20sampling%20from%20the%20dynamic)), while multi-agent learning algorithms that rely on naive evolutionary selection (e.g. replicator dynamics) can lead to **monocultures** of strategy. We propose an analogy to ecological **Lotka–Volterra (LV) dynamics** as a novel inference-time selection mechanism to preserve strategic and output diversity. The key insight is to introduce **intrinsic growth rates** – analogous to species’ innate growth – based on factors like novelty, quality heuristics, or computational cost, alongside usual fitness/payoff terms. This shifts selection dynamics away from pure payoff-proportional replication (as in standard replicator dynamics) to a richer dynamic that can maintain multiple high-quality candidates in equilibrium. We hypothesize that LV-based selection will outperform conventional decoding and policy selection methods by retaining a diverse set of **high-quality outputs**. We frame this hypothesis in light of current limitations of replicator dynamics and likelihood-maximization, outline how an LV-inspired algorithm can be implemented for LLM output filtering and multi-agent strategy selection, and discuss potential experiments. If validated, this approach would introduce a principled way to achieve diversity and **emergent behaviors** in AI systems by bridging concepts from ecology and evolutionary game theory.

## Introduction  
Large language models are typically deployed with decoding algorithms that aim to select the single “best” sequence according to the model’s probability or a learned reward. Unfortunately, maximizing likelihood at inference often yields suboptimal results: it has been observed that **greedy or beam search decoding leads to text degeneration**, producing outputs that are bland, repetitive, or incoherent ([The Curious Case of Neural Text Degeneration | OpenReview](https://openreview.net/forum?id=rygGQyrFvH#:~:text=language%20model%20%28e,distribution%2C%20sampling%20from%20the%20dynamic)). In other words, by always favoring the highest-probability continuation, the model tends to collapse onto a narrow subset of its vast generative capabilities. Similarly, in multi-agent or multi-strategy AI systems (e.g. self-play reinforcement learning or evolutionary algorithms), selection mechanisms that greedily favor the currently best-performing strategy can eliminate alternative strategies, reducing **population diversity** and potentially missing better solutions that require sustained exploration. This issue is analogous to ecological or evolutionary dynamics where a single fittest species can take over, driving others extinct, even if a diverse ecosystem would be more robust.

**Replicator dynamics**, a model from evolutionary game theory, provides a mathematical lens to understand this problem. Replicator dynamics assumes that each strategy’s reproductive success is proportional only to its payoff relative to the population average – effectively treating payoff (or analogously, log-likelihood) as the sole driver of selection ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=much%20like%20the%20original%201973,application%20process%2C%20but%20also%20a)). While replicator dynamics has been a dominant paradigm for modeling competition, it crucially assumes *identical intrinsic growth rates* for all strategies ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=much%20like%20the%20original%201973,application%20process%2C%20but%20also%20a)). In plain terms, aside from payoff differences, all individuals are treated equally; there is no inherent advantage or disadvantage to being a certain type in the absence of interactions. **This assumption is limiting**: it means that given enough time, even a slight payoff advantage will cause one strategy to completely dominate, often eliminating others. In the context of LLM decoding, this is analogous to always choosing the highest-probability tokens or sequences, which can lead to mode collapse and a loss of useful diversity in outputs.

Recent perspectives in evolutionary theory argue that these simplistic selection dynamics miss important factors. Tarnita and Traulsen (2025) note that because replicator dynamics neglects intrinsic differences, it cannot predict outcomes when strategies have different baseline growth capabilities ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=Specifically%2C%20if%20the%20types%20have,%E2%80%A1%20This%20remains%20true)) ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=Thus%2C%20accounting%20for%20intrinsic%20growth,growth%20rate%20of%20type%201)). In ecology, **Lotka–Volterra (LV) equations** generalize replicator dynamics by introducing intrinsic growth rates for each species (or strategy) in addition to interaction terms. Even a small intrinsic growth advantage can allow a species to persist despite competitive disadvantages, enabling stable coexistence of multiple types ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=reveals%20that%20changing%20the%20intrinsic,game%20theoretical%20classification%20largely%20uninformative)) ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=This%20means%20that%20an%20apparent,Thus%20type%201%20are)). By analogy, if each candidate output or agent strategy in an AI system had its own “intrinsic growth” value – reflecting attributes like novelty or independent merit – the selection process might favor a **balanced ecosystem of outputs** rather than a single dominant one.

Maintaining a population of diverse, high-quality candidates could address the shortcomings of current methods. Prior work in natural language generation has already highlighted the need for diversity: for example, *Diverse Beam Search (DBS)* modifies beam search to avoid producing near-duplicate sequences ([[1610.02424] Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/abs/1610.02424#:~:text=,by%20optimizing%20for%20a%20diversity)), and nucleus sampling was proposed to truncate low-probability tails and reduce repetitive gibberish ([](https://arxiv.org/pdf/1904.09751#:~:text=decoding%20methods%20such%20as%20beam,majority%20of%20the%20probability%20mass)). These techniques, however, are heuristic and not grounded in a unified selection dynamics framework. In multi-agent systems, practitioners have begun to encourage diversity explicitly – e.g. DeepMind’s AlphaStar league gave each agent variant a different objective or “bias” to promote a variety of strategies ([AlphaStar: Mastering the real-time strategy game StarCraft II - Google DeepMind](https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/#:~:text=To%20encourage%20diversity%20in%20the,objectives%20are%20adapted%20during%20training)). Such ad-hoc solutions hint that a more principled approach might exist.

In this paper, we introduce the hypothesis that **Lotka–Volterra dynamics can improve output filtering and strategy selection** in AI. By framing inference as an evolving population of candidate solutions (sentences, strategies, etc.), we can apply LV equations to govern which candidates thrive. We posit that adding intrinsic growth rates (for novelty, quality, etc.) to the selection process will preserve multiple **compelling outputs** instead of collapsing to one high-scoring but potentially mediocre output. The following sections will discuss related work, formalize our hypothesis, propose methods to implement LV-based selection in practice, and consider the implications and opportunities this approach opens for AI research.

## Related Work  

**Diversity in Language Model Decoding:** The tension between output quality and diversity in language generation is well-documented. Holtzman et al. (2020) demonstrated that standard maximum-likelihood decoding (greedy or beam search) often yields degenerate text – *“output that is bland, incoherent, or gets stuck in repetitive loops”* ([The Curious Case of Neural Text Degeneration | OpenReview](https://openreview.net/forum?id=rygGQyrFvH#:~:text=language%20model%20%28e,distribution%2C%20sampling%20from%20the%20dynamic)) – despite the underlying model being trained for high likelihood. To avoid this degeneration, stochastic methods like top-$k$ sampling and **nucleus sampling** (top-$p$ sampling) have been introduced ([](https://arxiv.org/pdf/1904.09751#:~:text=decoding%20methods%20such%20as%20beam,majority%20of%20the%20probability%20mass)). Beam search itself has been extended: Vijayakumar et al. (2018) introduced **Diverse Beam Search**, which maintains multiple beams that are penalized for similarity, thereby producing a list of candidates that differ more substantially ([[1610.02424] Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/abs/1610.02424#:~:text=,by%20optimizing%20for%20a%20diversity)). These methods can be seen as attempts to counteract the “survival of the fittest” nature of greedy decoding by injecting a bias for diversity. However, they typically do so via heuristic scoring adjustments or randomness, rather than a guided dynamic process. Our approach connects this problem to population dynamics, aiming to **systematically balance exploration and exploitation** via intrinsic growth terms in a selection equation.

**Replicator Dynamics in Multi-Agent Systems:** Evolutionary game theory provides tools to study how agent strategies evolve through repeated interactions. The replicator equation, in particular, has been used to interpret multi-agent reinforcement learning dynamics ([](https://arxiv.org/pdf/0904.4717#:~:text=a%20finite%20number%20of%20actions%2C,7%2C%2016%2C%2019%2C%2020%2C%2024%E2%80%9326)). In this analogy, each strategy or policy is akin to a species in a population, and strategies that yield higher payoff against the current mix increase in frequency. Indeed, researchers have shown that under certain conditions, the learning trajectories of Q-learning agents can be described by coupled replicator equations ([](https://arxiv.org/pdf/0904.4717#:~:text=a%20finite%20number%20of%20actions%2C,7%2C%2016%2C%2019%2C%2020%2C%2024%E2%80%9326)). While this provides insight, traditional replicator dynamics inherits the aforementioned limitation: it assumes no inherent difference between strategies beyond their interaction payoffs. If one strategy yields higher rewards, replicator dynamics will relentlessly favor it, which can lead to **convergence to a single dominant policy**. In complex games, however, it is known that a mixture of strategies is often necessary (there may be no single best strategy, as in rock-paper-scissors or StarCraft II’s diverse unit strategies). To address this, recent multi-agent training regimes use techniques to maintain a portfolio of strategies. For example, the **AlphaStar** league training maintained a population of agents with different goals or play-styles *“to encourage diversity in the league”* ([AlphaStar: Mastering the real-time strategy game StarCraft II - Google DeepMind](https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/#:~:text=To%20encourage%20diversity%20in%20the,objectives%20are%20adapted%20during%20training)), and ultimately combined them into a Nash mixture of strategies for play ([AlphaStar: Mastering the real-time strategy game StarCraft II - Google DeepMind](https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/#:~:text=training%2C%20each%20agent%20experienced%20up,on%20a%20single%20desktop%20GPU)). This is an empirical counterpart to what adding intrinsic growth rates could achieve in theory: it prevents one strategy from completely overtaking if others still offer unique strengths.

**Quality–Diversity Algorithms:** In evolutionary computation, **Quality-Diversity (QD)** algorithms explicitly seek a set of diverse solutions that are all high-performing, rather than a single optimum. These algorithms, such as MAP-Elites and Novelty Search with Local Competition, maintain an *archive* of solutions spanning different niches or behavior characteristics ([Quality Diversity: A New Frontier for Evolutionary Computation](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2016.00040/full#:~:text=Computation%20www,with%20respect%20to%20a)) ([[2502.00593] Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity](https://arxiv.org/abs/2502.00593#:~:text=%3E%20Abstract%3AQuality,tune)). They introduce the idea of *local competition*: individuals only compete strongly with those in similar niches, while diversity across niches is promoted. This yields an archive of solutions that are diverse yet each “elite” in its niche ([[2502.00593] Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity](https://arxiv.org/abs/2502.00593#:~:text=%3E%20Abstract%3AQuality,tune)). The motivations of QD align closely with our goal – avoiding the loss of valuable diversity while still selecting for quality. The LV dynamics we propose can be viewed as a continuous selection process embodying a similar principle: by using interaction terms (which can represent competition within similar outputs) and intrinsic growth (which can represent niche-specific fitness), we aim to analogously **preserve multiple niches of high-quality outputs** in one run of a generative model. Prior work on novelty search in RL and evolutionary algorithms has shown that rewarding novelty can lead to discovering solutions that pure objective-based search would miss, reinforcing our intuition that adding such intrinsic rewards (growth rates) is beneficial.

**Policy Gradient and RL Fine-Tuning:** An alternative to test-time selection is to adjust the generator itself via training, e.g. using policy gradient methods to better align with human preferences or other objectives (as done in Reinforcement Learning from Human Feedback, RLHF). Policy gradient training can indeed improve the quality of a single preferred output by optimizing expected reward, but it typically yields one optimized policy (the model parameters) rather than a set of diverse policies. Ensuring diversity in RL requires special care – e.g. entropy regularization to keep the policy stochastic or training multiple agents in parallel. In contrast, our focus is on **inference-time diversity**: without retraining the model, we can run a population-based selection over outputs. This complements RL-based fine-tuning; even a fine-tuned model can benefit from an LV-based decoder to avoid overconfident but possibly wrong or bland answers. Moreover, in settings like non-cooperative multi-agent games, policy gradient alone might oscillate or get stuck in cycles, whereas an explicitly population-based approach (like replicator or LV dynamics) can naturally capture cyclic or complex equilibria. Our work can be seen as bringing an RL-like notion of **intrinsic reward (novelty, exploration)** into the inference stage in a principled way, rather than requiring modifications to the training objective.

## Hypothesis  
We hypothesize that introducing **Lotka–Volterra dynamics** into the inference process of LLMs (and strategy selection of multi-agent systems) will **significantly improve diversity without sacrificing quality** of the outputs. In formal terms, consider a population of $N$ candidate solutions (these could be full sequences generated by an LLM or policies of agents in a game). Let $n_i(t)$ represent the *weight* or population share of the $i$-th candidate at time $t$ during the selection process. Conventional selection corresponds to **replicator dynamics**: each candidate’s growth rate is proportional to its *fitness* $f_i$ relative to the average fitness $\bar{f}$. This can be written as $\dot{n}_i = n_i \,(f_i - \bar{f})$. In the context of language generation, one might interpret $f_i$ as the log-likelihood or a heuristic score for candidate $i$; for agents, $f_i$ could be a payoff or reward. This replicator equation lacks any constant term, meaning if one candidate’s fitness is higher than others, its share will increase and others will decrease until possibly driven to zero. In fact, if one type has even a slightly higher $f_i$, eventually $n_i$ tends to 1 (fixation) and diversity is lost.

By contrast, we propose to use a **generalized Lotka–Volterra (LV) equation** of the form: $$\dot{n}_i \;=\; r_i\, n_i \;+\; \sum_{j=1}^{N} \alpha_{ij}\, n_i\, n_j,$$ where $r_i$ is an *intrinsic growth rate* for candidate $i$ and $\alpha_{ij}$ represents interaction coefficients between candidates $i$ and $j$. The intrinsic rate $r_i$ can be thought of as the fitness of $i$ **when rare and in isolation** ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=different%20intrinsic%20growth%20rates%20r,dynam%02ics%20of%20absolute%20abundances%2C%20ni)), i.e. the built-in merit of that candidate even before considering competition. The interaction term $\alpha_{ij}$ captures how candidates influence each other’s growth when both are present – analogous to competition or synergy in an ecosystem. For example, if two candidate answers are nearly identical (occupying the same niche), they might have a strong negative $\alpha_{ij}$ (competition), effectively preventing both from growing arbitrarily (one will eventually outcompete the other). On the other hand, if two candidates cover different aspects of a task (different niches), their interaction could be neutral or even mildly positive (they don’t impede each other’s survival).

Our hypothesis is that by appropriate design of $r_i$ and $\alpha_{ij}$, the selection dynamics will yield a **stable state containing multiple candidates** that are both high-quality and diverse. In particular, we expect **coexistence equilibria** to emerge, in contrast to the single-winner outcome of replicator dynamics. Evolutionary theory supports this expectation: *“changing the intrinsic growth rates relative to each other allows one to transform the dynamical outcome of any game”*, turning what was once dominance of one type into coexistence or other complex outcomes ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=reveals%20that%20changing%20the%20intrinsic,game%20theoretical%20classification%20largely%20uninformative)). For instance, an inferior strategy can survive if it has a sufficient intrinsic advantage to compensate for lower payoff ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=dynamical%20outcome%20of%20any%20game%3A,particularly%20pertinent%20to%20our%20main)) ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=This%20means%20that%20an%20apparent,Thus%20type%201%20are)). Translating this to LLM outputs: a candidate with slightly lower log-likelihood (payoff) might nonetheless persist through selection because it had a high novelty score ($r_i$) that gives it a growth boost. The end result would be that this novel but still reasonable candidate is not pruned away, yielding a final set of outputs that includes both the safe high-probability answer and more creative alternatives. We formally state our hypothesis as:

> **H1: Lotka–Volterra-based selection will maintain multiple diverse, high-quality outputs where conventional selection (replicator dynamics or likelihood-maximization) would converge to a single output.** In practice, this means an LV decoder for LLMs will produce a set of or final output that has higher diversity (measured by e.g. distinct-$n$ metrics or semantic variety) while matching or exceeding the quality of beam search outputs as judged by human or task-specific metrics. Likewise, in multi-agent scenarios, an LV-based strategy update will yield a mixture of strategies that collectively achieve higher robustness or reward than a single-strategy solution.

The introduction of **intrinsic growth rates** is the critical difference. These $r_i$ allow us to inject domain-specific preferences: e.g. $r_i$ could be positive for outputs that are novel (not too similar to any seen before), or incorporate a **quality heuristic** that is not captured by likelihood (such as grammaticality or factual correctness from an external evaluator). $r_i$ could also include negative components, such as a cost for length or computational complexity (analogous to a species with a high resource requirement having a lower baseline growth). By tuning $r_i$, we can bias the selection toward outputs that we *a priori* deem worth preserving (for diversity or quality reasons), even if their initial model score $f_i$ is not the highest. Meanwhile, the interaction terms $\alpha_{ij}$ can be used to discourage redundancy: if two outputs are too similar, they compete for the same niche and one will suppress the other’s growth. This is reminiscent of *niche overlap* competition in ecology and serves to ensure the final portfolio of outputs are meaningfully distinct. 

Crucially, our hypothesis does not claim that **all** outputs will survive or that quality is sacrificed for diversity. Instead, the idea is to strike a principled balance. In replicator dynamics, the balance is entirely tipped toward exploitation of the current best, whereas in LV dynamics the **intrinsic-extrinsic interplay** can stabilize a mix. The outcome should be a set of candidates that are each strong in some respect. We expect this to mirror results from quality-diversity research, where algorithms find many diverse high-performing solutions ([[2502.00593] Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity](https://arxiv.org/abs/2502.00593#:~:text=%3E%20Abstract%3AQuality,tune)) instead of one global optimum.

## Proposed Method  
To test and utilize this hypothesis, we outline a method to incorporate LV dynamics into the inference of language models and the strategy update of multi-agent systems. The method involves the following conceptual steps:

### 1. Candidate Generation (Initialization)  
For an LLM, first generate an initial pool of $N$ candidate outputs for a given prompt or task. This pool could be obtained via sampling (to ensure variability) or via beam search (to ensure baseline quality) or a combination (e.g. a few top beams plus some random samples). Each candidate is a sequence (e.g. a sentence or paragraph). For a multi-agent system, the “candidates” could be a set of $N$ different policies or strategy parameter vectors – for instance, strategies present in a population or archive at a given iteration of training, or distinct equilibrium candidates. Initially, each candidate $i$ is assigned a population weight $n_i(0)$ (for example, all equal $n_i(0)=1/N$ or weighted by another prior if we have one).

### 2. Defining Fitness and Intrinsic Growth  
We then define:  
- **Fitness (Interaction-based payoff) $f_i$:** a measure of how well candidate $i$ performs *relative to others*. In language generation, one might define $f_i$ simply as the base log-likelihood given by the model or a pseudo-reward (like a score from a neural evaluator if available). In multi-agent contexts, $f_i$ could come from pairwise evaluations (how strategy $i$ fares against strategy $j$ for all $j$) or a task-specific performance metric. These fitnesses will be used in the interaction terms of the LV equation.  
- **Intrinsic growth $r_i$:** a measure of the *inherent merit or bias* for candidate $i$, independent of the other candidates. This is where we encode properties like novelty and quality heuristics:
  - *Novelty:* We can compute a novelty score for each output, e.g. using an embedding-based diversity metric or n-gram uniqueness compared to the rest of the pool or a reference corpus. A highly novel or out-of-the-ordinary output might get a positive $r_i$. In multi-agent strategy, novelty could mean a strategy that is behaviorally very different from existing ones (to encourage exploring untried tactics).
  - *Quality heuristic:* We may have external checks or secondary models – for example, a rule-based grammar checker, a factual consistency checker for LLM outputs, or human preference model – which can provide a score for how “good” an output is in ways not captured by the base model’s likelihood. This score can contribute to $r_i$. For strategies, this could be a risk-versus-reward tradeoff or simplicity metric.
  - *Compute or complexity cost:* If certain outputs are extremely long or complex (or certain strategies are computationally heavy), we can assign a negative component to $r_i$ to mildly penalize them, preventing the process from being dominated by overly complex solutions. This is analogous to species with large body sizes having lower reproduction rates – a cost to balance their advantage.

Importantly, $r_i$ can be a weighted sum of these factors, tuned by hyperparameters that reflect how strongly we want to favor novelty vs. base likelihood, etc. One could even set $r_i=0$ for all $i$ to recover a baseline (no intrinsic differences, i.e. replicator-like behavior), which would be used as a control in experiments.

### 3. Defining Interaction Coefficients $\alpha_{ij}$  
We design $\alpha_{ij}$ to model competition or cooperation between candidates. A straightforward choice for output filtering is to use **pairwise similarity**: if outputs $i$ and $j$ are very similar (e.g. measured by semantic similarity or overlapping content), then $\alpha_{ij}$ and $\alpha_{ji}$ are set to a negative value (the candidates inhibit each other’s growth). This reflects that they are competing to fulfill the same role or answer – essentially a redundancy penalty. If outputs are dissimilar, we could set $\alpha_{ij}=0$ (no direct interaction) so they can coexist freely. More elaborate schemes could allow positive $\alpha_{ij}$ if two outputs together cover more ground (though in most cases we interpret interactions as competition for selection probability). For multi-agent strategies, interactions are naturally defined by game payoffs: if we have a payoff matrix or game outcome function $U(i,j)$ when strategy $i$ meets $j$, that can inform $\alpha_{ij}$. For example, in a zero-sum game, one strategy’s success is another’s loss (competitive interaction). In cooperative settings, strategies might have synergistic effects (could be modeled with positive interactions if we want to select a team of complementary agents).

In practice, to keep the system simple, we might not explicitly use a full $N \times N$ matrix of $\alpha_{ij}$. Instead, we could implement competition implicitly by **normalizing fitness relative to the population** at each step (as replicator does) or by using a simpler rule: e.g. *softmax selection with a diversity penalty*. One could derive an equivalent discrete-time update where at each iteration, we re-weight candidates by $\exp(f_i + r_i)$ and then normalize, which is analogous to a replicator with intrinsic growth in a time-discretized form. Additionally, one can implement the effect of $\alpha_{ij}$ by clustering or grouping similar candidates and capping their total share – this is a heuristic way to ensure not all top weight goes to very similar outputs.

### 4. Iterative Selection Process  
Using the defined $r_i$ and $\alpha_{ij}$, we simulate the selection dynamics for a few iterations. This could be done either by numerical integration of the LV equations or by an iterative algorithm:
  - **Continuous simulation:** Treat the $n_i$ as continuous and use small time steps $\Delta t$. Update each $n_i$ according to $\Delta n_i = n_i\,[\,r_i + \sum_j \alpha_{ij} n_j\,]\,\Delta t$. Ensure $n_i$ remain non-negative. In practice, one might simulate until a **quasi-equilibrium** is reached or for a fixed number of steps. 
  - **Reproduction-resampling algorithm:** An intuitive discrete approach is: at each “generation”, create offspring of each candidate proportional to $\exp(r_i + f_i)$ (or another function combining intrinsic and interaction fitness), then if two candidates are very similar, merge or prune one. This is conceptually similar to beam search but with a twist: we don’t always pick the top-$N$ outright; we allow some lower-ranked ones to persist due to $r_i$. Another approach: eliminate a fraction of the lowest-weight candidates and make copies of the highest-weight ones, then slightly perturb outputs (for LLMs, maybe allow them to continue generation or revise) and continue – this starts looking like an evolutionary algorithm for sequences.

For LLM decoding, a simpler proxy to running many iterations is to compute a **final score** for each candidate as $s_i = f_i + \lambda r_i$ (combining intrinsic and extrinsic) minus penalties for similarity, and then choose the top-scoring ones. This one-shot re-ranking with a diversity-aware score is easier to implement and can be seen as deriving from a single-step of the LV idea. However, a true iterative LV simulation might reveal more interesting dynamics (for example, oscillatory selection where two outputs take turns being dominant before settling, akin to ecological cycles – though in a static context like one prompt, a stable equilibrium is expected).

One notable feature of LV dynamics is the potential for **multiple stable equilibria or even cycles**, depending on parameters. In our use-case, this could mean the final set of outputs might depend on initial conditions or on subtle parameter choices. While this adds complexity, it could also be a feature: by adjusting $r_i$ or starting with different initial samples, we might obtain different diverse outputs, offering a richer exploration of the model’s output space. We will handle this carefully in experiments, possibly by averaging results over multiple runs or choosing parameters that produce a desired equilibrium.

### 5. Output Selection and Integration  
After the LV dynamics process, we will have a distribution $n_i$ over candidates. There are a few ways to use this outcome:
  - **Top Candidate Selection:** If the task requires a single output (as is common), we could simply choose the candidate with the highest $n_i$ (dominant species) after the process. The hope is that this winner is better than the greedy/beam search winner because it has survived a more rigorous and diversity-aware competition – possibly avoiding a bland but probable answer in favor of a more insightful one that also had strong intrinsic merit.
  - **Top-$k$ Diverse Outputs:** In many applications (like creative writing, multiple-choice QA, or providing options to a user), having several diverse high-quality outputs is valuable. We can take the top $k$ candidates by final weight $n_i$ and present them as the **diverse solution set**. This is effectively a more principled diverse beam. Each output in this set is expected to occupy a different niche (by design of our $\alpha_{ij}$) and to be of high quality (having either high base fitness or compensated by high $r_i$).
  - **Distribution Mixture:** In multi-agent training, the final $n_i$ could directly inform a mixed strategy. For example, instead of deploying a single policy, one might maintain a mixture (ensemble) of policies weighted by $n_i$. If the environment is non-stationary or adversarial, this mixture can be more robust (much like a mixed Nash strategy is unexploitable in zero-sum games).

The integration of this method into existing pipelines is also considered. For LLMs, an LV-based decoder could be a plug-in replacement for beam search or sampling in a generation API, toggled on when diversity is desired. For multi-agent systems, the LV update can be incorporated into population-based training loops or self-play leagues, possibly improving the exploration of strategy space.

## Implications and Discussion  
The proposed LV dynamic for inference carries several implications for research and practice:

- **Improved Diversity-Quality Tradeoff:** By leveraging intrinsic growth rates, we expect to **preserve useful diversity** in model outputs. This means, for example, a dialogue system might offer replies that are not only highly relevant but also one that is more creative or provocative if novelty is rewarded. In tasks like story generation, this could prevent the common pitfall of all endings being similar, instead yielding genuinely different story endings for the same prompt. Importantly, those diverse outputs are not random – they are all judged to be viable (high fitness) in their own way, just as an ecosystem’s species each occupy a viable niche. This could substantially enrich user experiences with AI, providing multi-faceted results from a single query. From a metrics perspective, we anticipate higher diversity scores (such as Distinct-1/2, or embedding coverage) **without loss in task-specific metrics** like BLEU or human preference scores, compared to beam search ([[1610.02424] Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/abs/1610.02424#:~:text=also%20typically%20fails%20to%20capture,visual%20question%20generation%20using%20both)).

- **Mitigating Mode Collapse in RLHF and Alignment:** Recent fine-tuning of LLMs with human feedback has sometimes resulted in safe but **homogenized** model responses (often called *mode collapse* towards a single style). An LV-based decoding could act as a safeguard, introducing an internal pressure for alternatives. Even if the reward model strongly prefers a particular style, giving a small intrinsic boost to different styles or voices could surface outputs that a purely reward-maximizing approach would suppress. This may help in aligning AI that doesn’t always give the same canned response, thereby maintaining richness in dialogue and avoiding the system becoming overly narrow in behavior.

- **Emergent Behaviors and Open-Ended Exploration:** One exciting possibility is that when using LV dynamics, especially in multi-agent systems, we might observe **emergent cycles or diverse equilibria**. For instance, if agents have strategies that dominate each other in a rock-paper-scissors fashion, a replicator dynamic might cycle indefinitely or eventually focus on one due to drift. But an LV dynamic with slight intrinsic bonuses for the weaker strategies could stabilize a *polymorphic equilibrium* where all strategies persist ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=dynamical%20outcome%20of%20any%20game%3A,particularly%20pertinent%20to%20our%20main)). This means the system as a whole continually exhibits a variety of behaviors, potentially leading to richer interaction patterns. In creative domains, one could imagine setting up multiple LLM “agents” that propose ideas and critique each other; an LV selection among their ideas could maintain a balance where no agent’s style completely suppresses the others, fostering a kind of conversational or ideational diversity. Such dynamics could be a rudimentary model of **ecosystems of agents or ideas**, opening avenues to study open-ended generation and innovation in AI. The influence of intrinsic novelty resembles an *intrinsic motivation* in reinforcement learning – encouraging the exploration of the state or strategy space for its own sake, which is known to produce more inventive solutions.

- **Comparisons to Existing Search Methods:** It’s worth noting that if one tunes the LV parameters to extremes, it recovers known methods. For example, if $r_i$ is made very large for all candidates equally (dominating the interaction term), then essentially all candidates will grow regardless of fitness – analogous to pure sampling (high randomness). If $r_i=0$ and interactions are fully competitive proportional to fitness, we recover replicator (and in discrete sense, beam search). Thus, our framework encompasses a spectrum between deterministic exploitation and exploratory diversity. It provides a *single framework* to interpolate between these extremes, which researchers could analyze theoretically. For instance, one could study the stability of the equilibrium and derive conditions for when a certain number of candidates will coexist. This might give deeper understanding of why certain diversity heuristics work (e.g. it could possibly re-derive the effect of Diverse Beam Search’s penalty in terms of an LV interaction coefficient).

- **Practical Considerations:** Implementing LV dynamics introduces new hyperparameters (the $r_i$ weighting scheme, interaction strengths, number of iterations, etc.). This complexity must be managed so that the method is robust. We suggest conducting ablation studies in experiments: e.g. vary the magnitude of novelty bonus $r$, measure the impact on diversity and quality, and identify regimes that yield improvements. There might be diminishing returns – too high a novelty bonus could keep truly poor outputs in play, hurting overall quality. We anticipate a **sweet spot** where $r_i$ is large enough to protect useful diversity but not so large as to elevate garbage output. Automatic tuning of these parameters (perhaps via gradient-based optimization on a validation set or even meta-learning) could be explored. Additionally, the computational overhead of maintaining a population needs evaluation. If $N$ (number of candidates) is large and we run many iterations, inference could slow down. However, one can likely achieve good results with modest $N$ (dozens) and few iterations, since the intrinsic factors guide selection efficiently. In batch generation settings (where multiple outputs are generated in parallel on modern hardware), having 10–20 candidates is often feasible.

- **Evaluation and Experimentation:** To validate our hypothesis, we propose experiments on a few fronts. *For language models:* use tasks such as open-ended story generation, dialog response, or summarization where diversity is pertinent. Compare outputs from: (a) standard beam search, (b) nucleus sampling, (c) diverse beam search, and (d) our LV-based method. Evaluation would include automatic metrics for diversity (e.g. number of distinct unigrams/bigrams, entropy of outputs) and quality (perplexity, BLEU if reference, or human preference via A/B testing). We expect (d) to match (a) in coherence while exceeding it in diversity, and to produce more consistently relevant outputs than (b) which can sometimes drift off-topic. *For multi-agent systems:* design a simple game or use a benchmark (e.g. iterative prisoners’ dilemma, or a cooperative task requiring role differentiation). Use a population of agents updated either by replicator dynamics or by LV dynamics with intrinsic rewards for exploring strategies. Measure diversity of strategies (how many distinct policies remain in play) and team performance or equilibrium payoff. We anticipate that with LV, multiple strategies will persist and the system’s performance will be as good as or better than the single-strategy case, due to having a repertoire to handle varied situations. A particularly telling experiment would be to introduce an environmental change or a new adversary after training – a population with diverse strategies should adapt more robustly than a monomorphic one.

- **Broader Impact:** Embracing an ecological perspective in AI could have broad ramifications. It encourages us to think beyond optimization for a single outcome and towards maintaining an **adaptive population** of solutions. This could be beneficial for AI safety, as a diverse set of models/outputs might be less likely to all fail on the same corner case. It also might inspire *interdisciplinary collaboration*: ideas from theoretical ecology (e.g. niche theory, predator-prey dynamics, etc.) might inform new algorithms for exploration, while insights from AI’s complex systems could loop back to ecology and evolutionary biology. In the long run, one could imagine AI systems that **self-diversify** their outputs or behaviors to avoid stagnation, much like ecosystems thrive through biodiversity.

In conclusion, the Lotka–Volterra dynamics hypothesis offers a compelling new tool for AI researchers and practitioners. By **infusing inference with intrinsic growth rates and interaction-driven selection**, we aim to overcome the current limitations of replicator-like selection in large language models and multi-agent systems. This approach stands to produce AI outputs that are not only **high-quality** in the narrow sense, but also **strategically diverse and rich**, capturing more of the full spectrum of possibilities the model or agents can generate. We encourage further exploration of this idea through theoretical analysis and empirical validation, as it holds promise for advancing the state of the art in generative AI and multi-agent learning.

## References (Excerpt)  
- Ari Holtzman *et al.*, **“The Curious Case of Neural Text Degeneration,”** *ICLR 2020*. (Demonstrating degeneration in maximum-likelihood decoding and introducing nucleus sampling)  ([The Curious Case of Neural Text Degeneration | OpenReview](https://openreview.net/forum?id=rygGQyrFvH#:~:text=language%20model%20%28e,distribution%2C%20sampling%20from%20the%20dynamic)) ([](https://arxiv.org/pdf/1904.09751#:~:text=decoding%20methods%20such%20as%20beam,majority%20of%20the%20probability%20mass)).  
- Ashwin K. Vijayakumar *et al.*, **“Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models,”** *AAAI 2018*. (Proposing diversity-augmented beam search to capture inherent ambiguity in generation)  ([[1610.02424] Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/abs/1610.02424#:~:text=,by%20optimizing%20for%20a%20diversity)).  
- Corina E. Tarnita and Arne Traulsen, **“Reconciling ecology and evolutionary game theory or ‘When not to think cooperation’,”** *PNAS 2025*. (Argues that identical intrinsic growth rates in replicator dynamics limit its scope, and shows how different intrinsic rates yield coexistence and other outcomes)  ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=much%20like%20the%20original%201973,application%20process%2C%20but%20also%20a)) ([When not to think cooperation.pdf](file://file-SevYgYxTcZZPsFQmi8rHzS#:~:text=reveals%20that%20changing%20the%20intrinsic,game%20theoretical%20classification%20largely%20uninformative)).  
- Ryan Bahlous-Boldi *et al.*, **“Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity,”** *arXiv 2025*. (Overview of Quality-Diversity algorithms that maintain diverse, high-performing solutions via local competition)  ([[2502.00593] Dominated Novelty Search: Rethinking Local Competition in Quality-Diversity](https://arxiv.org/abs/2502.00593#:~:text=%3E%20Abstract%3AQuality,tune)).  
- OpenAI et al., **“AlphaStar: Mastering the Real-Time Strategy Game StarCraft II,”** *DeepMind blog 2019*. (Describes the AlphaStar league training; each agent had tailored objectives to ensure a diverse set of strategies in the final league)  ([AlphaStar: Mastering the real-time strategy game StarCraft II - Google DeepMind](https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/#:~:text=To%20encourage%20diversity%20in%20the,objectives%20are%20adapted%20during%20training)).  
- Aram Galstyan, **“Continuous Strategy Replicator Dynamics for Multi–Agent Learning,”** *arXiv 2011*. (Draws parallels between multi-agent learning and replicator dynamics, illustrating the use of evolutionary game theory in multi-agent reinforcement learning)  ([](https://arxiv.org/pdf/0904.4717#:~:text=a%20finite%20number%20of%20actions%2C,7%2C%2016%2C%2019%2C%2020%2C%2024%E2%80%9326)).