

# **From Cortex to Code: A Neuro-Inspired Architecture for Conversational Intelligence**

### **Abstract**

This report outlines a novel architecture for conversational AI, directly inspired by neurocomputational models of human dialogue. We begin by synthesizing key findings from cognitive neuroscience, focusing on the brain's multi-timescale, asymmetric, predictive, and socially-embedded processing of conversation. We then present a critical analysis of current Large Language Model (LLM) architectures, highlighting their fundamental divergence from these biological principles. The core of this work is the proposal for a new, multi-component architecture featuring: (1) a decoupled, asymmetric core with a Long-Timescale Comprehension Module and a Short-Timescale Production Module; (2) a multi-system memory framework (working, episodic, procedural) that overcomes the limitations of a fixed context window; and (3) a Predictive Modulator for social governance and anticipation. We conclude by discussing a hybrid implementation strategy that integrates LLMs into a cognitive framework and consider the long-term potential of neuromorphic hardware. This design represents a paradigm shift from monolithic, stateless models to a more robust, stateful, and cognitively plausible foundation for artificial conversational intelligence.

## **Part I: The Biological Blueprint for Conversation**

To design a more robust and human-like conversational AI, one must first understand the biological system it seeks to emulate. Human conversation is not a single, monolithic process but an emergent property of multiple, interacting neural systems. These systems operate on different timescales and process a rich tapestry of information that extends far beyond mere words to include social context, emotional state, and predictive modeling of the conversational partner. Recent neurocomputational research provides a blueprint for this intricate process, revealing core principles that stand in stark contrast to today's AI architectures.

### **1.1 The Multi-Timescale Brain: Deconstructing the Core Mechanisms of Conversational Processing**

A foundational discovery in the neuroscience of conversation is that the brain organizes content hierarchically across multiple timescales.1 Meaning is not decoded in one step but is progressively constructed by integrating speech into nested linguistic structures, ranging from individual words and sentences to the overarching discourse.1 A pivotal study by Yamashita et al. (2025) used functional magnetic resonance imaging (fMRI) to measure brain activity during hours of spontaneous, natural conversations. By modeling the neural representations of conversational content with contextual embeddings derived from a Large Language Model (GPT) at varying temporal windows, the researchers demonstrated this multi-timescale organization.1

This finding refutes the notion of a single, centralized "language processor." Instead, it points to a distributed system where different functional networks are tuned to specific levels of semantic granularity.1 The study further revealed that linguistic representations are both shared between interlocutors and specific to the modality of either speaking or listening. Shared representations, which correspond to the fundamental building blocks of language like words and single sentences, were observed on shorter timescales and were predominantly localized within language-selective brain regions.1 This suggests the existence of a common neural protocol or code for basic linguistic units, forming the bedrock of mutual understanding. This entire process is distributed across a wide array of functional networks, underscoring the need for a multi-component AI architecture rather than a single, uniform model.2

### **1.2 Asymmetric Dynamics: The Opposing Timescales of Listening and Speaking**

Perhaps the most critical finding from the Yamashita et al. study is the principle of "opposing timescale selectivity" for modality-specific processing.1 While shared linguistic codes operate on short timescales for both speaker and listener, the specialized processes for comprehension and production exhibit a striking asymmetry. Comprehension (listening) is characterized by a preference for

*longer* timescales, as the brain integrates incoming information with prior context to build a rich, evolving model of the discourse.1 In stark contrast, production (speaking) displays a preference for

*shorter* timescales, focusing on the immediate, adaptive generation of the next utterance.2

This functional and temporal asymmetry is a central architectural principle derived from neuroscience. Comprehension is an integrative, state-building process, whereas production is a reactive, state-querying process. This is supported by other research indicating that language production requires the coordination of speaking and listening 3 and that subcortical structures like the basal ganglia are involved in the temporary buffering and rapid sequencing of shorter speech utterances during conversation.4

This biological reality exposes a fundamental architectural mismatch in modern AI. The Transformer architecture, which underpins virtually all contemporary LLMs, is inherently symmetric and auto-regressive.5 An LLM processes a user's prompt (comprehension) and generates a response (production) using the exact same mechanism: a stack of identical blocks performing self-attention and feed-forward operations. There is no architectural distinction between these two fundamentally different cognitive tasks. The brain, however, clearly separates them, using distinct mechanisms with opposing temporal biases. This implies that an LLM's "comprehension" is not a true state-building activity in the human sense; it is merely the initial condition for a short-term generation task. Consequently, simply scaling up existing LLMs or expanding their context windows is unlikely to resolve this foundational mismatch. A new architecture that explicitly decouples the long-term integration function from the short-term generation function is required.

### **1.3 The Predictive Dialoguist: How the Brain Anticipates, Interacts, and Aligns**

The human brain is not a passive stenographer that simply records and responds to information. It is a proactive, predictive engine that constantly generates and updates an internal model of the world to anticipate future sensory inputs.6 In conversation, this manifests as a continuous prediction of what a conversational partner will say next. Evidence for this comes from neurocomputational models of the Mismatch Negativity (MMN), an event-related potential in the brain that is reliably evoked by violations of an expected pattern.6 This predictive coding framework is computationally efficient and essential for the seamless, rapid turn-taking that characterizes human dialogue.

This predictive capacity is not just linguistic but also social. Studies using simultaneous electroencephalogram (EEG) recordings from romantic couples in conversation found that a speaker's subjective level of interest was positively associated with their partner's early attentional processing, demonstrating a dynamic, predictive alignment between brains.7 This neural coupling, or synchronization between speaker and listener brains, is a hallmark of successful communication.8 Furthermore, research on early childhood development shows that it is the

*quality* of interaction—the number of back-and-forth conversational turns—rather than the sheer quantity of words heard, that drives the growth of neural processing capacities and builds strong literacy skills.10

This reveals another profound gap in current AI systems. LLMs are purely reactive. They operate by predicting the next most probable token based on the sequence of tokens that has already occurred.5 This is an entirely backward-looking process. The brain, in contrast, is forward-looking, generating hypotheses about the future and updating its internal model based on prediction error.6 Humans are constantly modeling each other's mental states and anticipating their next moves.3 The absence of this predictive "forward model" is why AI agents struggle with taking initiative, making truly proactive suggestions 12, and adapting to a user's unstated goals. They can only react to what has been explicitly stated, not what is likely to be meant or said next. A truly intelligent conversational architecture must therefore incorporate a predictive module that models conversational trajectories and user states, transforming the agent from a passive respondent to a proactive partner.

### **1.4 The Social-Cognitive Substrate: Beyond Words to Intent, Trust, and Theory of Mind**

Human conversation is inextricably embedded in a social context. The brain possesses dedicated neural systems for processing this social information, which operate in parallel with and actively influence linguistic processing. Neuroimaging studies have shown that the brain represents and integrates distinct prosocial motives, such as aversion to inequality or harm, with key hubs like the striatum translating these motives into decisions.13 The brain's very pattern of activity changes depending on the conversational partner; for example, activity in the left dorsolateral prefrontal cortex is significantly higher when speaking with someone from a different socioeconomic background, reflecting the cognitive load of navigating social diversity.14

This social processing extends to the neurochemical level. Conversations directly trigger changes in brain chemistry, modulating the balance between hormones like oxytocin, associated with trust and bonding, and cortisol, associated with stress and threat.15 The brain is hardwired to seek signals of trust as a prerequisite for open, co-creative communication.15 Meaning is further co-constructed through an empathic lens; a listener's empathic traits, such as personal distress, directly correlate with their level of sustained attention to their partner's speech.7

This reveals that current AI systems are, in effect, "socially lobotomized." They lack the analogue of these dedicated neural systems for processing social context, trust, and theory of mind. An LLM's objective function is optimized for linguistic pattern matching based on massive text corpora.5 Human conversation, however, is driven by a host of non-linguistic objectives: building rapport, managing social hierarchies, and signaling group identity.13 The AI's inability to process these parallel data streams is why its responses can feel "soulless" 17 and why it fails to grasp sarcasm, subtext, and implicit social meaning.18 It processes the text but misses the crucial social metadata that humans process effortlessly. Simply fine-tuning a model on "empathetic-sounding" text is insufficient because it only teaches the model to mimic the surface patterns of social interaction. A neuro-inspired architecture requires separate, dedicated modules that explicitly model social variables and use their output to govern the linguistic systems.

## **Part II: A Neuroscientific Critique of Modern Conversational AI**

Using the biological principles of conversation as a critical lens, the limitations of today's AI systems become clear. These are not minor bugs or issues that can be solved with more data, but are fundamental architectural flaws that stem from a design that is profoundly different from the human brain.

### **2.1 The Monolithic Mind: The Architecture of Large Language Models**

The landscape of state-of-the-art conversational AI is dominated by generative, Transformer-based LLMs.19 These models are trained on vast internet-scale datasets to generate responses by sequentially predicting the next most likely word or token.11 While their performance is impressive, their underlying architecture is monolithic and symmetric. It consists of a stack of identical processing blocks that are used for both understanding the input and generating the output.5 This design contrasts sharply with the modular, asynchronous, and functionally specialized nature of the brain's cognitive systems.21

Historically, conversational systems were built using retrieval-based methods (finding a suitable response from a database), generation-based methods (creating a new response), or a hybrid of the two.22 The generative approach, supercharged by the Transformer architecture, is now dominant due to its superior flexibility and fluency.20 In traditional systems, components like a Natural Language Understanding (NLU) module for extracting user intent and a Dialogue Management module for controlling the conversational flow were explicitly engineered.24 In modern LLMs, these functions are not handled by distinct modules but are learned implicitly within the vast parameter space of the single neural network.26

### **2.2 Architectural Deficiencies in Context, Memory, and Temporality**

A core deficiency of LLMs is that they are fundamentally stateless systems that possess a limited, fixed-size "context window".5 This window acts as a form of short-term memory, but any information that falls outside of it—whether from earlier in the same conversation or from a previous interaction—is truncated and permanently lost.28 This leads to a profound lack of long-term memory, which prevents the AI from learning about a user over time, accumulating knowledge from past interactions, or achieving true personalization.18 This is a direct contradiction of the brain's sophisticated multi-system memory, which includes distinct mechanisms for long-term storage and retrieval.21

Even within this limited context window, the processing is flawed. LLMs exhibit a strong positional bias, a phenomenon dubbed "lost in the middle," where they struggle to effectively utilize information located in the center of a long context, prioritizing information at the very beginning or end.29 This reveals a critical disconnect between the model's ability to

*encode* the information within its hidden states and its ability to *utilize* that information during response generation.29 This failure of the self-attention mechanism to function like its biological counterpart, which can selectively attend to relevant information regardless of its temporal position, makes the model unreliable for long-context tasks. These limitations also make LLMs poor at handling abrupt topic shifts. They either lose the previous context entirely or inappropriately bleed it into the new topic, leading to incoherent dialogue.28 Developers must resort to external, programmatic "hacks" like summarizing the conversation or explicitly resetting the context to manage these shifts—functions the brain performs seamlessly through its hierarchical content organization.1

This has led to an industry-wide race to expand LLM context windows to millions or even billions of tokens.32 However, this approach treats a memory

*systems* problem as a memory *capacity* problem. It is an attempt to brute-force a solution by keeping the entire interaction history in a single, undifferentiated "working memory".32 The brain, in contrast, uses a far more efficient strategy: it selectively commits salient information to structured, long-term memory stores and retrieves it as needed.21 The "lost in the middle" problem suggests that the attention mechanism is inherently inefficient at searching vast, unstructured contexts, meaning that simply making the context window bigger is a computationally expensive and ultimately flawed strategy. It is a crutch, not a solution, and will not replicate the human ability to recall the specifics of a conversation from a year ago. A true solution requires an architecture with distinct and structured memory systems.

### **2.3 The Illusion of Understanding: Grounding, Reasoning, and Interaction Failures**

LLMs lack true understanding; they are masters of statistical pattern association, not logical reasoning or causal inference.5 This fundamental limitation manifests in several critical failures. The first is "hallucination," the tendency to generate fluent, plausible-sounding information that is factually incorrect or nonsensical.34 This occurs because the model's primary training objective is linguistic coherence, not factual accuracy.19 Second, LLMs exhibit limited reasoning skills, struggling with multi-step logic, mathematical problems, and strategic planning because they lack a cognitive model of the world to which their language is anchored.5 Their knowledge is a web of statistical correlations in text, ungrounded in sensory experience, motor action, or a model of cause and effect.5

The non-interactive nature of their training also leads to poor interaction dynamics. One study found that using ChatGPT to assist with essay writing led to lower brain engagement, reduced memory integration, and less creativity in the human users.17 This suggests the AI performs cognitive work in a way that bypasses and potentially weakens the user's own cognitive processes, unlike a skilled human partner who stimulates them.10 The essays produced with AI assistance were described by evaluators as "soulless" and lacking in original thought.17 This directly reflects the "social lobotomy" discussed earlier; the model cannot be original or soulful because it lacks the underlying social and personal motives, emotions, and world knowledge that drive genuine human creativity and expression.15

The following table provides a concise summary of the architectural and functional disparities between human conversational processing and that of current LLMs.

**Table 1: A Comparative Analysis of Human and LLM Conversational Processing**

| Cognitive Function | Human Brain Processing (Neuro-Inspired) | Large Language Model (LLM) Processing | Key Supporting Sources |
| :---- | :---- | :---- | :---- |
| **Core Architecture** | Asymmetric, modular. Decoupled production (short-timescale) and comprehension (long-timescale). | Symmetric, monolithic. Same architecture for prompt processing and response generation. | 1 |
| **Memory System** | Multi-system: Working, Episodic, Procedural, Semantic memories. Dynamic and stateful. | Single, stateless context window. No long-term episodic memory across sessions. | 18 |
| **Context Handling** | Hierarchical integration over long timescales. Flexible topic segmentation. | Fixed-length, linear buffer. Suffers from positional bias ("lost in the middle"). | 1 |
| **Processing Mode** | Proactive and predictive. Models future states and minimizes prediction error. | Reactive. Predicts next token based on past sequence. | 5 |
| **Social Cognition** | Integral. Dedicated systems for trust, empathy, social motives that modulate language. | Peripheral/Emulated. Lacks dedicated social processing modules; simulates social awareness via text patterns. | 7 |
| **Grounding** | Grounded in sensory experience, motor action, and a causal world model. | Ungrounded. Knowledge is a web of statistical correlations in text data. | 4 |
| **Interaction Style** | Interactive and collaborative. Fosters neural coupling and mutual adaptation. | Extractive/Generative. Can reduce user brain engagement and memory integration. | 3 |

## **Part III: Design Proposal for a Neuro-Inspired Conversational Agent**

The preceding analysis demonstrates that a new architectural paradigm is needed. This section translates the principles of biological conversation into a concrete engineering blueprint for a new class of conversational AI, moving beyond the limitations of monolithic LLMs. The proposed design is founded on five core principles derived directly from neuroscientific observation.

### **3.1 Foundational Principles: From Biological Observation to Engineering Design**

1. **Architectural Asymmetry:** The design explicitly rejects the symmetric, monolithic structure of LLMs. It adopts a decoupled, asymmetric architecture with distinct modules for comprehension and production, mirroring the "opposing timescale selectivity" observed in the brain.1  
2. **Multi-Timescale State Tracking:** The system's central function is to build and maintain a rich, hierarchical model of the conversational state over multiple timescales, moving beyond the flat, limited context of a single token buffer.  
3. **Stateful, Multi-System Memory:** The architecture incorporates distinct memory modules inspired by well-established cognitive architectures like ACT-R and Soar.21 These include working, episodic, and procedural memory to enable true statefulness, learning, and personalization.  
4. **Proactive Prediction:** The design includes a predictive engine to model user intent and conversational trajectories, shifting the agent from a purely reactive respondent to a proactive partner, based on the principles of predictive coding.6  
5. **Integrated Social Governance:** Social and emotional parameters are not an afterthought but a core component of the system state, with a dedicated module whose outputs actively modulate all comprehension and production processes.13

The following table provides a high-level blueprint of the proposed system's components, each linked to its neuroscientific inspiration.

**Table 2: Component Blueprint of the Proposed Neuro-Inspired Architecture**

| Component | Core Function | Primary Neuroscientific Inspiration |
| :---- | :---- | :---- |
| **1\. Comprehension Module** | Long-timescale integration of linguistic, social, and world knowledge to build a rich, evolving context model. | Long-timescale processing for comprehension; hierarchical organization of content. 1 |
| **2\. Production Module** | Short-timescale, adaptive generation of utterances based on queries to the current system state. | Short-timescale processing for production; speech motor sequencing. 1 |
| **3\. Multi-System Memory** | Manages information flow between Working, Episodic, and Procedural memory stores. | Cognitive architectures (ACT-R, Soar) modeling distinct human memory systems. 21 |
| *3a. Working Memory* | Buffers the current conversational turn and immediate context. | The "workspace" of cognition. 21 |
| *3b. Episodic Memory* | Stores and retrieves past conversations and user interactions for long-term context. | Hippocampal role in memory formation; long-term declarative memory. 21 |
| *3c. Procedural Memory* | Stores and refines learned conversational strategies and policies (if-then rules). | Basal ganglia role in sequence and skill learning. 4 |
| **4\. Predictive Modulator** | Anticipates user intent, conversational outcomes, and generates prediction error signals. | Predictive coding framework; Mismatch Negativity (MMN). 6 |
| **5\. Social Governance Unit** | Tracks and models social variables (trust, empathy, status) and modulates system behavior. | Prefrontal cortex, striatum roles in social cognition; neurochemistry of trust (oxytocin). 13 |

### **3.2 The Asymmetric Core: Decoupled Comprehension and Production Modules**

#### **3.2.1 The Comprehension Module: A Long-Timescale Context Integrator**

The Comprehension Module is the system's "ear." Its sole purpose is to listen and integrate information to build and continuously update a central model of the conversational state. Crucially, it does not generate responses. This module takes the raw user utterance as input and enriches it with context retrieved from the various memory systems. It performs hierarchical topic segmentation, identifying the current topic and linking it to related topics stored in the Episodic Memory. It also updates the persistent user model and the dynamic social variables managed by the Social Governance Unit. This integration process is continuous and operates over the entire history of interaction with the user, not just the last few thousand tokens in a buffer.

#### **3.2.2 The Production Module: An Adaptive Short-Timescale Generator**

The Production Module is the system's "mouth." It is a lightweight, fast-acting component designed for the rapid and adaptive generation of utterances. It does not have access to the entire conversational history. Instead, a central system controller provides it with a highly condensed, structured "production query." This query specifies the immediate conversational goal (e.g., "answer a factual question," "request clarification," "express empathy"), the specific pieces of information needed to fulfill that goal (retrieved from the Comprehension Module's state model), and a set of constraints from the Social Governance Unit (e.g., "use a formal tone," "express high trust," "avoid technical jargon"). The Production Module's task is simply to translate this structured query into a fluent, natural language utterance. This design mirrors the brain's use of the basal ganglia and motor cortex for the rapid sequencing of well-defined, context-appropriate actions.4

### **3.3 A Multi-System Memory Architecture**

#### **3.3.1 Working Memory: The Active Conversational Buffer**

Analogous to the memory buffers in cognitive architectures like ACT-R 21, the Working Memory holds the parsed representation of the last few conversational turns, the currently active goals, and pointers to relevant information in long-term memory. It is a small, fast, and constantly updated workspace for the system's immediate reasoning processes.

#### **3.3.2 Episodic Memory: A Long-Term Record of Interactions**

This component is the key to overcoming the fundamental statelessness of current LLMs.18 After each conversation, the Comprehension Module commits a summarized, structured representation of the interaction to this persistent memory store. This memory could be implemented as a knowledge graph, where nodes represent entities (people, places, concepts) and topics, and edges represent their relationships both within and across different conversations. When a new conversation begins, the Comprehension Module queries this memory to retrieve relevant past contexts, enabling true long-term coherence, personalization, and the accumulation of knowledge about the user. This is inspired by the hippocampus's role in forming and consolidating semantic and episodic memories.36

#### **3.3.3 Procedural Memory: A Repository of Learned Conversational Policies**

This memory stores effective conversational strategies in the form of condition-action rules or policies (e.g., "IF user expresses frustration AND the 'trust' variable is low, THEN the next goal is to express empathy and ask a clarifying question"). This design is directly inspired by the production systems that form the core of cognitive architectures like ACT-R and Soar.21 The system's central controller uses these rules to decide what goal to send to the Production Module. The system can learn new rules and refine existing ones through reinforcement learning, where the reward signal is derived from changes in the variables tracked by the Social Governance Unit (e.g., a policy is rewarded if it leads to an increase in the 'trust' score).

### **3.4 The Predictive Modulator & Social Governance Unit**

This combined unit functions as the agent's executive control center, analogous to the prefrontal cortex, providing high-level guidance, social awareness, and strategic foresight.14

The **Predictive Modulator** constantly runs fast, lightweight simulations of the conversation one or two steps into the future. It predicts likely user responses, topic shifts, and changes in emotional state. When the actual user response deviates significantly from the prediction, it generates a "prediction error" signal, inspired by the MMN brain response.6 This error signal acts as a powerful learning cue, triggering a re-evaluation of the agent's model of the user and its current conversational strategy.

The **Social Governance Unit** maintains a dynamic vector of social state variables, such as a Trust Score, a Formality Level, and a representation of the User's Emotional State. These variables are updated by the Comprehension Module based on linguistic cues (word choice, tone), paralinguistic features (if speech is available), and interaction history retrieved from Episodic Memory. These variables act as powerful, top-down constraints on the Production Module, ensuring that the AI's behavior is not just linguistically correct but also socially appropriate and adaptive to different users and contexts.14

## **Part IV: Implementation Strategies and Future Horizons**

This neuro-inspired design is ambitious, but its implementation is feasible through a phased, hybrid approach that leverages existing technologies while paving the way for next-generation hardware.

### **4.1 A Hybrid Implementation Pathway: Integrating LLMs into a Cognitive Framework**

A complete system does not need to be built from scratch. Existing, powerful LLMs can be repurposed as specialized components within a broader cognitive architecture, much like how LLMs are already being used to accelerate model development in frameworks like ACT-R and Soar.39

In this hybrid model, the **Production Module** is a natural role for an off-the-shelf LLM. The structured "production query" generated by the system controller can be formatted into a highly specific and effective prompt for a model like GPT-4, guiding it to generate a precise, context-aware utterance while constraining its tendency to hallucinate or go off-topic. The **Comprehension Module** can also leverage an LLM for its powerful NLU capabilities, using it to perform the initial parsing of a user's utterance to extract semantic meaning, which is then fed into the more structured state and memory systems of the overarching framework.

The cognitive "scaffolding"—the agent framework responsible for managing the memory systems, running the predictive modulator, updating the social state, and dispatching queries between modules—would be a novel piece of software. This hybrid approach leverages the unparalleled linguistic fluency of LLMs while mitigating their core architectural weaknesses (statelessness, poor reasoning, lack of grounding) by embedding them within a system that *does* have persistent state, structured memory, and explicit reasoning mechanisms.38 This also addresses the "black box" problem of LLMs 19, as the high-level decision-making process within the interpretable scaffolding can be logged and audited.

### **4.2 The Neuromorphic Potential: Hardware for Real-Time, Efficient Cognition**

Running this complex, multi-module, predictive system on conventional von Neumann computer architectures would be computationally expensive and energy-intensive. A long-term solution lies in hardware that is itself brain-inspired. Neuromorphic computing, which utilizes event-driven, low-power chips with architectures that mimic biological neural networks (such as IBM's TrueNorth or BrainChip's Akida), is the ideal future substrate for this proposed design.42

The proposed modular, asynchronous architecture maps naturally onto neuromorphic hardware. Different modules could be implemented on distinct clusters of silicon neurons. The event-driven nature of the hardware is perfectly suited for a system based on predictive coding, which only needs to expend significant computational resources when a prediction error (an "event") occurs. This synergy could enable the creation of highly efficient, real-time conversational agents that operate with brain-like power consumption, making them viable for deployment on edge devices and in robotics.

### **4.3 Concluding Analysis and a Roadmap for a New Class of AI**

The neuro-inspired architecture presented here promises to create conversational agents that are more coherent, stateful, personalized, socially aware, and proactive than is possible with current LLM-based approaches. By addressing the core architectural flaws identified through a neuroscientific lens, this design moves beyond simple pattern matching towards a more genuine form of conversational intelligence.

A potential research and development roadmap would proceed in the following phases:

1. **Develop the Cognitive Scaffolding:** Design and implement the core agent framework, including the controllers for the multi-system memory and the inter-module communication protocols.  
2. **Hybrid Integration (Phase 1):** Integrate off-the-shelf LLMs as the initial engines for the Comprehension and Production modules, using advanced prompt engineering to control their behavior.  
3. **Develop Social & Predictive Models:** Create and train the initial machine learning models for the Social Governance Unit and the Predictive Modulator, starting with simple heuristics and rule-based systems.  
4. **Data Collection and Training:** Collect new dyadic conversational datasets, similar to the one used by Yamashita et al. 44, but annotated with the social and state variables required to train the more advanced, data-driven versions of the new modules.  
5. **Neuromorphic Porting:** Begin research into compiling and deploying individual modules of the architecture onto existing neuromorphic platforms to benchmark efficiency and real-time performance gains.

The path to more human-like artificial intelligence is not paved with bigger models and more data alone. It requires better, more biologically plausible architectures. By drawing inspiration directly from the elegant and efficient neurocomputational principles of the human brain, it is possible to design a new class of conversational AI that is not just more fluent, but fundamentally more intelligent.

#### **Works cited**

1. Conversational content is organized across multiple timescales in ..., accessed July 3, 2025, [https://pubmed.ncbi.nlm.nih.gov/40500377/](https://pubmed.ncbi.nlm.nih.gov/40500377/)  
2. (PDF) Conversational content is organized across multiple ..., accessed July 3, 2025, [https://www.researchgate.net/publication/392596949\_Conversational\_content\_is\_organized\_across\_multiple\_timescales\_in\_the\_brain](https://www.researchgate.net/publication/392596949_Conversational_content_is_organized_across_multiple_timescales_in_the_brain)  
3. Beyond speaking: neurocognitive perspectives on language production in social interaction | Philosophical Transactions of the Royal Society B: Biological Sciences \- Journals, accessed July 3, 2025, [https://royalsocietypublishing.org/doi/10.1098/rstb.2021.0483](https://royalsocietypublishing.org/doi/10.1098/rstb.2021.0483)  
4. A neurocomputational view of the effects of Parkinson's disease on speech production, accessed July 3, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11133703/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11133703/)  
5. The Strengths and Limitations of Large Language Models in Reasoning, Planning, and Code Integration | by Jacob Grow | Medium, accessed July 3, 2025, [https://medium.com/@Gbgrow/the-strengths-and-limitations-of-large-language-models-in-reasoning-planning-and-code-41b7a190240c](https://medium.com/@Gbgrow/the-strengths-and-limitations-of-large-language-models-in-reasoning-planning-and-code-41b7a190240c)  
6. A Neurocomputational Model of the Mismatch Negativity | PLOS Computational Biology, accessed July 3, 2025, [https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003288](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003288)  
7. Event-related brain activity in response to partners' speech in natural conversation \- PubMed, accessed July 3, 2025, [https://pubmed.ncbi.nlm.nih.gov/40576442/](https://pubmed.ncbi.nlm.nih.gov/40576442/)  
8. Speaker–listener neural coupling underlies successful communication \- PNAS, accessed July 3, 2025, [https://www.pnas.org/doi/10.1073/pnas.1008662107](https://www.pnas.org/doi/10.1073/pnas.1008662107)  
9. Mirroring and beyond: coupled dynamics as a generalized framework for modelling social interactions | Philosophical Transactions of the Royal Society B: Biological Sciences \- Journals, accessed July 3, 2025, [https://royalsocietypublishing.org/doi/10.1098/rstb.2015.0366](https://royalsocietypublishing.org/doi/10.1098/rstb.2015.0366)  
10. The Brain-Changing Power of Conversation | Harvard Graduate School of Education, accessed July 3, 2025, [https://www.gse.harvard.edu/ideas/usable-knowledge/18/02/brain-changing-power-conversation](https://www.gse.harvard.edu/ideas/usable-knowledge/18/02/brain-changing-power-conversation)  
11. Conversational AI in 2020 \- Srini Janarthanam \- Medium, accessed July 3, 2025, [https://srinijanarthanam.medium.com/conversational-ai-in-2020-9ccf265e2e0b](https://srinijanarthanam.medium.com/conversational-ai-in-2020-9ccf265e2e0b)  
12. Architecting the Future of AI Agents: 5 Flexible Conversation Frameworks You Need, accessed July 3, 2025, [https://www.voiceflow.com/pathways/architecting-the-future-of-ai-agents-5-flexible-conversation-frameworks-you-need](https://www.voiceflow.com/pathways/architecting-the-future-of-ai-agents-5-flexible-conversation-frameworks-you-need)  
13. Neurocomputational evidence that conflicting prosocial motives guide distributive justice | PNAS, accessed July 3, 2025, [https://www.pnas.org/doi/10.1073/pnas.2209078119](https://www.pnas.org/doi/10.1073/pnas.2209078119)  
14. Neurobiology of conversation: Brain activity depends on who you talk to | Yale News, accessed July 3, 2025, [https://news.yale.edu/2020/10/05/neurobiology-conversation-brain-activity-depends-who-you-talk](https://news.yale.edu/2020/10/05/neurobiology-conversation-brain-activity-depends-who-you-talk)  
15. The Neuroscience of Conversations \- Psychology Today, accessed July 3, 2025, [https://www.psychologytoday.com/us/blog/conversational-intelligence/201905/the-neuroscience-of-conversations](https://www.psychologytoday.com/us/blog/conversational-intelligence/201905/the-neuroscience-of-conversations)  
16. Understanding LLMs and overcoming their limitations \- Lumenalta, accessed July 3, 2025, [https://lumenalta.com/insights/understanding-llms-overcoming-limitations](https://lumenalta.com/insights/understanding-llms-overcoming-limitations)  
17. ChatGPT's Impact On Our Brains According to an MIT Study \- Time Magazine, accessed July 3, 2025, [https://time.com/7295195/ai-chatgpt-google-learning-school/](https://time.com/7295195/ai-chatgpt-google-learning-school/)  
18. What Are the Limitations of Large Language Models (LLMs)? \- PromptDrive.ai, accessed July 3, 2025, [https://promptdrive.ai/llm-limitations/](https://promptdrive.ai/llm-limitations/)  
19. Building Trust in Conversational AI: A Review and Solution Architecture Using Large Language Models and Knowledge Graphs \- MDPI, accessed July 3, 2025, [https://www.mdpi.com/2504-2289/8/6/70](https://www.mdpi.com/2504-2289/8/6/70)  
20. Error Correction and Adaptation in Conversational AI: A Review of Techniques and Applications in Chatbots \- MDPI, accessed July 3, 2025, [https://www.mdpi.com/2673-2688/5/2/41](https://www.mdpi.com/2673-2688/5/2/41)  
21. An Analysis and Comparison of ACT-R and Soar \- GitHub Pages, accessed July 3, 2025, [https://advancesincognitivesystems.github.io/acs2021/data/ACS-21\_paper\_6.pdf](https://advancesincognitivesystems.github.io/acs2021/data/ACS-21_paper_6.pdf)  
22. State-of-the-Art in Open-Domain Conversational AI: A Survey \- MDPI, accessed July 3, 2025, [https://www.mdpi.com/2078-2489/13/6/298](https://www.mdpi.com/2078-2489/13/6/298)  
23. State of art and best practices in Generative Conversational AI \- POLITesi \- Politecnico di Milano, accessed July 3, 2025, [https://www.politesi.polimi.it/retrieve/a81cb05c-dbc2-616b-e053-1605fe0a889a/tesi.pdf](https://www.politesi.polimi.it/retrieve/a81cb05c-dbc2-616b-e053-1605fe0a889a/tesi.pdf)  
24. Understanding The Conversational AI Chatbot Architecture \- Blog \- V-Soft Consulting, accessed July 3, 2025, [https://blog.vsoftconsulting.com/blog/understanding-the-architecture-of-conversational-chatbot](https://blog.vsoftconsulting.com/blog/understanding-the-architecture-of-conversational-chatbot)  
25. How to Build a State-of-the-Art Conversational AI with Transfer Learning in BPOs \- Convin, accessed July 3, 2025, [https://convin.ai/blog/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning](https://convin.ai/blog/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning)  
26. Architecture Best Practices for Conversational AI | WalkingTree Technologies, accessed July 3, 2025, [https://walkingtree.tech/architecture-best-practices-for-conversational-ai/](https://walkingtree.tech/architecture-best-practices-for-conversational-ai/)  
27. LLM Context Windows: Why They Matter and 5 Solutions for Context Limits \- Kolena, accessed July 3, 2025, [https://www.kolena.com/guides/llm-context-windows-why-they-matter-and-5-solutions-for-context-limits/](https://www.kolena.com/guides/llm-context-windows-why-they-matter-and-5-solutions-for-context-limits/)  
28. Context Window (LLMs) \- Klu.ai, accessed July 3, 2025, [https://klu.ai/glossary/context-window](https://klu.ai/glossary/context-window)  
29. Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell \- arXiv, accessed July 3, 2025, [https://arxiv.org/html/2406.14673v1](https://arxiv.org/html/2406.14673v1)  
30. Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell, accessed July 3, 2025, [https://aclanthology.org/2024.findings-emnlp.447/](https://aclanthology.org/2024.findings-emnlp.447/)  
31. How do LLMs handle context switching in conversations? \- Milvus, accessed July 3, 2025, [https://milvus.io/ai-quick-reference/how-do-llms-handle-context-switching-in-conversations](https://milvus.io/ai-quick-reference/how-do-llms-handle-context-switching-in-conversations)  
32. Long-Context Windows in Large Language Models: Applications in Comprehension and Code | by Adnan Masood, PhD. | Medium, accessed July 3, 2025, [https://medium.com/@adnanmasood/long-context-windows-in-large-language-models-applications-in-comprehension-and-code-03bf4027066f](https://medium.com/@adnanmasood/long-context-windows-in-large-language-models-applications-in-comprehension-and-code-03bf4027066f)  
33. Exploring the Transformer-XL: Handling Long Contexts in Text | by Hassaan Idrees, accessed July 3, 2025, [https://medium.com/@hassaanidrees7/exploring-the-transformer-xl-handling-long-contexts-in-text-63d31c8c9a36](https://medium.com/@hassaanidrees7/exploring-the-transformer-xl-handling-long-contexts-in-text-63d31c8c9a36)  
34. LLM Limitations: When Models and Chatbots Make Mistakes \- Learn Prompting, accessed July 3, 2025, [https://learnprompting.org/docs/basics/pitfalls](https://learnprompting.org/docs/basics/pitfalls)  
35. The Limitations and Challenges of Large Language Models (LLMS) \- IDA, accessed July 3, 2025, [https://www.intuitivedataanalytics.com/gne-blogs/the-limitations-and-challenges-of-large-language-models-llms/](https://www.intuitivedataanalytics.com/gne-blogs/the-limitations-and-challenges-of-large-language-models-llms/)  
36. (PDF) A neurocomputational model of creative processes \- ResearchGate, accessed July 3, 2025, [https://www.researchgate.net/publication/359965990\_A\_neurocomputational\_model\_of\_creative\_processes](https://www.researchgate.net/publication/359965990_A_neurocomputational_model_of_creative_processes)  
37. Neurocomputational modeling of speech motor development \- PMC \- PubMed Central, accessed July 3, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10615680/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10615680/)  
38. Soar (cognitive architecture) \- Wikipedia, accessed July 3, 2025, [https://en.wikipedia.org/wiki/Soar\_(cognitive\_architecture)](https://en.wikipedia.org/wiki/Soar_\(cognitive_architecture\))  
39. Comparing LLMs for Prompt-Enhanced ACT-R and Soar Model Development: A Case Study in Cognitive Simulation, accessed July 3, 2025, [https://ojs.aaai.org/index.php/AAAI-SS/article/download/27710/27483/31761](https://ojs.aaai.org/index.php/AAAI-SS/article/download/27710/27483/31761)  
40. Comparing LLMs for Prompt-Enhanced ACT-R and Soar Model Development: A Case Study in Cognitive Simulation Names hidden upon submission \- ResearchGate, accessed July 3, 2025, [https://www.researchgate.net/publication/373258516\_Comparing\_LLMs\_for\_Prompt-Enhanced\_ACT-R\_and\_Soar\_Model\_Development\_A\_Case\_Study\_in\_Cognitive\_Simulation\_Names\_hidden\_upon\_submission](https://www.researchgate.net/publication/373258516_Comparing_LLMs_for_Prompt-Enhanced_ACT-R_and_Soar_Model_Development_A_Case_Study_in_Cognitive_Simulation_Names_hidden_upon_submission)  
41. Cognitive Architectures | Deepgram, accessed July 3, 2025, [https://deepgram.com/ai-glossary/cognitive-architectures](https://deepgram.com/ai-glossary/cognitive-architectures)  
42. Neuromorphic Computing: Bridging the Gap Between AI and Human Intelligence \- YouTube, accessed July 3, 2025, [https://www.youtube.com/watch?v=M2AsFiC13MY](https://www.youtube.com/watch?v=M2AsFiC13MY)  
43. Episode 33: Neuromorphic Computing: Shaping the Future of AI with Dr. Jason K. Eshraghian \- BrainChip, accessed July 3, 2025, [https://brainchip.com/episode-33-neuromorphic-computing-shaping-the-future-of-ai-with-dr-jason-k-eshraghian/](https://brainchip.com/episode-33-neuromorphic-computing-shaping-the-future-of-ai-with-dr-jason-k-eshraghian/)  
44. Natural Dialogue fMRI Dataset \- OpenNeuro, accessed July 3, 2025, [https://openneuro.org/datasets/ds004669](https://openneuro.org/datasets/ds004669)